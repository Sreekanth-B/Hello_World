{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Chatbot Development with Python NLTK:\\n\\nChatbots are intelligent agents that engage in a conversation with the humans in order to answer user queries on a certain topic.\\n\\nAmazon’s Alexa, Apple’s Siri and Microsoft’s Cortana are some of the examples of chatbots.\\n\\nDepending upon the functionality, chatbots can be divided into three categories: General purpose chatbots, task-oriented chatbots,\\nand hybrid chatbots. General purpose chatbots are the chatbots that conduct a general discussion with the user \\n(not on any specific topic). Task-oriented chatbots, on the other hand, are designed to perform specialized tasks, \\nfor example, to serve as online ticket reservation system or pizza delivery system, etc. Finally, hybrid chatbots are designed \\nfor both general and task-oriented discussions.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Chatbot Development with Python NLTK:\n",
    "\n",
    "Chatbots are intelligent agents that engage in a conversation with the humans in order to answer user queries on a certain topic.\n",
    "\n",
    "Amazon’s Alexa, Apple’s Siri and Microsoft’s Cortana are some of the examples of chatbots.\n",
    "\n",
    "Depending upon the functionality, chatbots can be divided into three categories: General purpose chatbots, task-oriented chatbots,\n",
    "and hybrid chatbots. General purpose chatbots are the chatbots that conduct a general discussion with the user \n",
    "(not on any specific topic). Task-oriented chatbots, on the other hand, are designed to perform specialized tasks, \n",
    "for example, to serve as online ticket reservation system or pizza delivery system, etc. Finally, hybrid chatbots are designed \n",
    "for both general and task-oriented discussions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chatbot Development Approaches\\n\\nThere are two major approaches for developing chatbots: Rule-based approaches and learning-based approaches.\\nRule-based Approaches\\nIn rule-based approaches, there is a fixed set of responses available and based on a certain rule, a response is selected. For instance, if a user says “hello”, there might be an if statement in the chatbot that implements the logic that  whenever a user says “hello”, generate a response which says “hi, how are you?”One of the advantages of the rule-based approach is that they are often very correct since there is a perfect response to a query. However, rule-based chatbots do not scale well and cannot give reply to user inputs for which no rule is defined. In order to answer a large number of user queries, a large number of rules have to be implemented.\\n\\nLearning-based Approaches\\nLearning-based approaches use statistical algorithms such as Machine Learning algorithms to learn from the data and generate responses based on that learning. One of the advantages of learning based approaches is that they scale well. However, learning based approaches require a huge amount of data to train and may not be very accurate.\\n\\nLearning-based approaches have been further divided into two categories: Generative approaches and Retrieval approaches. Generative chatbots are the type of chatbots that learn to generate words in response to a user query. On the other hand, retrieval chatbots learn a complete response to be generated as a result of user inputs. Generative approaches are more flexible as compared to retrieval approaches as depending upon the user input, a response is generated on the fly.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Chatbot Development Approaches\n",
    "\n",
    "There are two major approaches for developing chatbots: Rule-based approaches and learning-based approaches.\n",
    "Rule-based Approaches\n",
    "In rule-based approaches, there is a fixed set of responses available and based on a certain rule, a response is selected. For instance, if a user says “hello”, there might be an if statement in the chatbot that implements the logic that  whenever a user says “hello”, generate a response which says “hi, how are you?”One of the advantages of the rule-based approach is that they are often very correct since there is a perfect response to a query. However, rule-based chatbots do not scale well and cannot give reply to user inputs for which no rule is defined. In order to answer a large number of user queries, a large number of rules have to be implemented.\n",
    "\n",
    "Learning-based Approaches\n",
    "Learning-based approaches use statistical algorithms such as Machine Learning algorithms to learn from the data and generate responses based on that learning. One of the advantages of learning based approaches is that they scale well. However, learning based approaches require a huge amount of data to train and may not be very accurate.\n",
    "\n",
    "Learning-based approaches have been further divided into two categories: Generative approaches and Retrieval approaches. Generative chatbots are the type of chatbots that learn to generate words in response to a user query. On the other hand, retrieval chatbots learn a complete response to be generated as a result of user inputs. Generative approaches are more flexible as compared to retrieval approaches as depending upon the user input, a response is generated on the fly.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Developing a Chatbot in Python\n",
    "\n",
    "#In this tutorial, we will develop a very simple task-oriented chatbot system capable of answer questions related to global warming. The chatbot will be fairly simple and will generate answers based on cosine similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Downloading Required Libraries\\nBefore we can proceed with the code, we need to download the following libraries:\\n\\nChatbot development falls in the broader category of Natural Language processing. We will be using a natural language processing library NLTK to create our chatbot.  The installation instructions for NLTK can be found at this official link.(https://www.nltk.org/install.html)\\nThe dataset used for creating our chatbot will be the Wikipedia article on global warming (https://en.wikipedia.org/wiki/Global_warming). To scrape the article, we will use the BeautifulSoap library for Python. The download instructions for the library are available here(https://pypi.org/project/beautifulsoup4/).\\nThe BeautifulSoap library scrapes the data from a website in HTML format. To parse the HTML, we will use the LXML library. The download instructions for the library are available at official link.(https://lxml.de/)\\nWe will be using the Anaconda distribution of Python, the rest of the libraries are built-in the Anaconda distribution and you do not have to download them.\\n\\nLet’s now start our chatbot development. We need to perform the following steps:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Downloading Required Libraries\n",
    "Before we can proceed with the code, we need to download the following libraries:\n",
    "\n",
    "Chatbot development falls in the broader category of Natural Language processing. We will be using a natural language processing library NLTK to create our chatbot.  The installation instructions for NLTK can be found at this official link.(https://www.nltk.org/install.html)\n",
    "The dataset used for creating our chatbot will be the Wikipedia article on global warming (https://en.wikipedia.org/wiki/Global_warming). To scrape the article, we will use the BeautifulSoap library for Python. The download instructions for the library are available here(https://pypi.org/project/beautifulsoup4/).\n",
    "The BeautifulSoap library scrapes the data from a website in HTML format. To parse the HTML, we will use the LXML library. The download instructions for the library are available at official link.(https://lxml.de/)\n",
    "We will be using the Anaconda distribution of Python, the rest of the libraries are built-in the Anaconda distribution and you do not have to download them.\n",
    "\n",
    "Let’s now start our chatbot development. We need to perform the following steps:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    " \n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In the script above we import the beautifulsoup4 library for parsing the webpage, the urllib library to make a connection to a remote webpage, the re  library for performing regex operation, the nltk  library for natural language processing, and the numpy  library for basic array operations. The random  library is used for random number generation. We will see how we use it later in the article. And finally, the string  library is used for string manipulation.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' In the script above we import the beautifulsoup4 library for parsing the webpage, the urllib library to make a connection to a remote webpage, the re  library for performing regex operation, the nltk  library for natural language processing, and the numpy  library for basic array operations. The random  library is used for random number generation. We will see how we use it later in the article. And finally, the string  library is used for string manipulation.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraping  and Preprocessing the Wikipedia Article\\nOnce we imported the required libraries, we are good to go and scrape the article from Wikipedia. As I said earlier, our chatbot will be able to answer questions related to global warming. To develop such a chatbot, we need a dataset that contains information about global warming. One of the sources of such information is the Wikipedia article on global warming. The following script scrapes the article and extracts paragraphs from the article.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Scraping  and Preprocessing the Wikipedia Article\n",
    "Once we imported the required libraries, we are good to go and scrape the article from Wikipedia. As I said earlier, our chatbot will be able to answer questions related to global warming. To develop such a chatbot, we need a dataset that contains information about global warming. One of the sources of such information is the Wikipedia article on global warming. The following script scrapes the article and extracts paragraphs from the article.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming')  \n",
    "raw_data = raw_data.read()\n",
    " \n",
    "html_data = bs.BeautifulSoup(raw_data,'lxml')\n",
    " \n",
    "all_paragraphs =html_data.find_all('p')\n",
    " \n",
    "article_content = \"\"\n",
    " \n",
    "for p in all_paragraphs:  \n",
    "    article_content += p.text\n",
    "    \n",
    "article_content =  article_content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the script above we first use the urlopen  function from the urllib.request module to fetch raw data from Wikipedia. Next, we use the read method to read the data. The data retrieved using the urlopen function is in binary format, to convert it into HTML format, we can use the BeautifulSoup class and pass it our raw data along with the lxml  object, as parameters. From the HTML data, we need to extract the paragraphs, we can do so using the find_all  method. We need to pass p  as the parameter to the method which stands for paragraphs. Finally, we joined all the paragraphs and converted the final text into lowercase.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''In the script above we first use the urlopen  function from the urllib.request module to fetch raw data from Wikipedia. Next, we use the read method to read the data. The data retrieved using the urlopen function is in binary format, to convert it into HTML format, we can use the BeautifulSoup class and pass it our raw data along with the lxml  object, as parameters. From the HTML data, we need to extract the paragraphs, we can do so using the find_all  method. We need to pass p  as the parameter to the method which stands for paragraphs. Finally, we joined all the paragraphs and converted the final text into lowercase.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will remove numbers from our dataset and replace multiple empty spaces with single space. This step is optional, you can skip it.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We will remove numbers from our dataset and replace multiple empty spaces with single space. This step is optional, you can skip it.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = re.sub(r'\\[[0-9]*\\]', ' ', article_content ) \n",
    "article_content = re.sub(r'\\s+', ' ', article_content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next step is to tokenize the article into sentences and words. Tokenizations simply refers to splitting the article.\\n\\nThe following script tokenizes the article into sentences:'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The next step is to tokenize the article into sentences and words. Tokenizations simply refers to splitting the article.\n",
    "\n",
    "The following script tokenizes the article into sentences:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qx816/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the following script tokenizes the article into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_words= nltk.word_tokenize(article_content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemmatization and Punctuation Removal\\nLemmatization refers to reducing the word to its root form, as available in the dictionary. For instance, the lemmatized version of the word eating will be eat , better will be good , medium will be media and so on.\\n\\nLemmatization helps find similarity between the words since similar words can be used in different tense and different degrees. Lemmatizing them makes them uniform.\\n\\nSimilarly, we will remove punctuations from our text because punctuations do not convey any meaning and if we do not remove them, they will also be treated as tokens.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lemmatization and Punctuation Removal\n",
    "Lemmatization refers to reducing the word to its root form, as available in the dictionary. For instance, the lemmatized version of the word eating will be eat , better will be good , medium will be media and so on.\n",
    "\n",
    "Lemmatization helps find similarity between the words since similar words can be used in different tense and different degrees. Lemmatizing them makes them uniform.\n",
    "\n",
    "Similarly, we will remove punctuations from our text because punctuations do not convey any meaning and if we do not remove them, they will also be treated as tokens.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We will use NLTK’s punkt  and wordnet modules for punctuation removal. We can then use the WordNetLemmatizer object from the nltk.stem  module for lemmatizing the words.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' We will use NLTK’s punkt  and wordnet modules for punctuation removal. We can then use the WordNetLemmatizer object from the nltk.stem  module for lemmatizing the words.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    " \n",
    "def LemmatizeWords(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    " \n",
    "remove_punctuation= dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
    " \n",
    "def RemovePunctuations(text):\n",
    "    return LemmatizeWords(nltk.word_tokenize(text.lower().translate(remove_punctuation)))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In the script above two helper functions, LemmatizeWords and RemovePunctuations  have been defined. The RemovePunctuations  function accepts a text string, perform lemmatization on the string by passing it to LemmatizeWords function which lemmatizes the words. Punctuations are also removed from the text.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' In the script above two helper functions, LemmatizeWords and RemovePunctuations  have been defined. The RemovePunctuations  function accepts a text string, perform lemmatization on the string by passing it to LemmatizeWords function which lemmatizes the words. Punctuations are also removed from the text.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Handling Greetings:\\nThe Wikipedia article doesn’t contain any text for handling greetings, however, we want our chatbot to reply to greetings. For that, we will create a function that handles greetings. Basically, we will create two lists with different types of greeting messages. The user input will be checked against the words in one of the greeting lists, if the user input contains a word that is in the first greeting list, the response will be randomly chosen from the second greeting list.  The following script does that:'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Handling Greetings:\n",
    "The Wikipedia article doesn’t contain any text for handling greetings, however, we want our chatbot to reply to greetings. For that, we will create a function that handles greetings. Basically, we will create two lists with different types of greeting messages. The user input will be checked against the words in one of the greeting lists, if the user input contains a word that is in the first greeting list, the response will be randomly chosen from the second greeting list.  The following script does that:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "greeting_input_texts = (\"hey\", \"heys\", \"hello\", \"morning\", \"evening\",\"greetings\",)\n",
    "greeting_replie_texts = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello there\", \"ello\", \"Welcome, how are you\"]\n",
    " \n",
    "def reply_greeting(text):\n",
    " \n",
    "    for word in text.split():\n",
    "        if word.lower() in greeting_input_texts:\n",
    "            return random.choice(greeting_replie_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response Generation\\nNext, we need to create a method for general response generation. To do so we need to convert our words to vectors or numbers and then apply cosine similarity to find the similar vectors. The intuition behind this approach is that the response words should have the highest cosine similarity with user input words. To convert word to vectors we will use TF-IDF approach . We can use TfidfVectorizer  from the sklearn.feature_extraction.text  module to convert words to their TF-IDF counterparts.  Similarly, to find the cosine similarity, the  cosine_similarity method from the sklearn.metrics.pairwise  class can be used.  The following script imports these modules:'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Response Generation\n",
    "Next, we need to create a method for general response generation. To do so we need to convert our words to vectors or numbers and then apply cosine similarity to find the similar vectors. The intuition behind this approach is that the response words should have the highest cosine similarity with user input words. To convert word to vectors we will use TF-IDF approach . We can use TfidfVectorizer  from the sklearn.feature_extraction.text  module to convert words to their TF-IDF counterparts.  Similarly, to find the cosine similarity, the  cosine_similarity method from the sklearn.metrics.pairwise  class can be used.  The following script imports these modules:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_reply(user_input):\n",
    "    chatbot_response=''\n",
    "    sentence_list.append(user_input)\n",
    "    word_vectors = TfidfVectorizer(tokenizer=RemovePunctuations, stop_words='english')\n",
    "    vecrorized_words = word_vectors.fit_transform(sentence_list)\n",
    "    similarity_values = cosine_similarity(vecrorized_words[-1], vecrorized_words)\n",
    "    similar_sentence_number =similarity_values.argsort()[0][-2]\n",
    "    similar_vectors = similarity_values.flatten()\n",
    "    similar_vectors.sort()\n",
    "    matched_vector = similar_vectors[-2]\n",
    "    if(matched_vector ==0):\n",
    "        chatbot_response=chatbot_response+\"I am sorry! I don't understand you\"\n",
    "        return chatbot_response\n",
    "    else:\n",
    "        chatbot_response = chatbot_response +sentence_list[similar_sentence_number]\n",
    "        return chatbot_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The above function simply takes user input as a parameter, lemmatizes, removes punctuations, and then create TFIDF vectors from the words in the sentence. TFIDF vectors for the already existing sentences in the article is also created. Next, cosine similarity between vectors of the words in the sentence entered by the user and the existing sentences is found and the sentence with the highest cosine similarity is returned as a response. In case if no cosine similarity is found between user input and any sentence in the article, the response is generated that the sentence is not understood.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The above function simply takes user input as a parameter, lemmatizes, removes punctuations, and then create TFIDF vectors from the words in the sentence. TFIDF vectors for the already existing sentences in the article is also created. Next, cosine similarity between vectors of the words in the sentence entered by the user and the existing sentences is found and the sentence with the highest cosine similarity is returned as a response. In case if no cosine similarity is found between user input and any sentence in the article, the response is generated that the sentence is not understood.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interacting with User:\\nNow we have created a method for user interaction. We need to create logic to interact with the user. Look at the following method:'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Interacting with User:\n",
    "Now we have created a method for user interaction. We need to create logic to interact with the user. Look at the following method:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/qx816/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the script above, we set a flag continue_discussion  to True. Next,  execute a while loop inside which we ask the user to input his/her questions regarding global warming.  The loop executes until the  continue_discussion  flag is set to True. If the user input is equal to the string ‘bye’, the loop terminates by setting  continue_discussion  flag to False . Else if the user input contains words like thank ‘thanks’, ‘thank you very much’ or ‘thank you’ the response generated will be ‘Chatbot: Most welcome’.  If the user input contains a greeting, the response generated will contain greeting. Finally, if the user input doesn’t contain ‘bye’ or ‘thank you’ words or greetings, the user input is sent to give_reply function that we created in the last section, the function returns an appropriate response based on cosine similarity.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''In the script above, we set a flag continue_discussion  to True. Next,  execute a while loop inside which we ask the user to input his/her questions regarding global warming.  The loop executes until the  continue_discussion  flag is set to True. If the user input is equal to the string ‘bye’, the loop terminates by setting  continue_discussion  flag to False . Else if the user input contains words like thank ‘thanks’, ‘thank you very much’ or ‘thank you’ the response generated will be ‘Chatbot: Most welcome’.  If the user input contains a greeting, the response generated will contain greeting. Finally, if the user input doesn’t contain ‘bye’ or ‘thank you’ words or greetings, the user input is sent to give_reply function that we created in the last section, the function returns an appropriate response based on cosine similarity.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a chatbot, I will answer your queries regarding global warming:\n",
      "Hey\n",
      "Chatbot: Welcome, how are you\n",
      "good\n",
      "Chatbot: they challenged the scientific evidence, argued that global warming would have benefits, warned that concern for global warming was some kind of socialist plot to undermine american capitalism, and asserted that proposed solutions would do more harm than good.\n",
      "what is global warming\n",
      "Chatbot: global warming refers to global averages, with the amount of warming varying by region.\n"
     ]
    }
   ],
   "source": [
    "continue_discussion=True\n",
    "print(\"Hello, I am a chatbot, I will answer your queries regarding global warming:\")\n",
    "while(continue_discussion==True):\n",
    "    user_input = input()\n",
    "    user_input = user_input .lower()\n",
    "    if(user_input !='bye'):\n",
    "        if(user_input =='thanks' or user_input =='thank you very much'  or user_input =='thank you'):\n",
    "            continue_discussion=False\n",
    "            print(\"Chatbot: Most welcome\")\n",
    "        else:\n",
    "            if(reply_greeting(user_input)!=None):\n",
    "                print(\"Chatbot: \"+reply_greeting(user_input))\n",
    "            else:\n",
    "                print(\"Chatbot: \",end=\"\")\n",
    "                print(give_reply(user_input))\n",
    "                sentence_list.remove(user_input)\n",
    "    else:\n",
    "        continue_discussion=False\n",
    "        print(\"Chatbot: Take care, bye ..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
