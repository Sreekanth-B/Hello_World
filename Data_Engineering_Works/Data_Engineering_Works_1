

To Run .hql files from hive ::(.hql files you can write sql commands)

hive -f /home/<path to hive file>/filename.hql
hive -f /home/<path to hive file>/filename.hql
hive -f /home/<path to hive file>/filename.hql
=====================================================================================================================================


# Executing the .hql files in the unix environment

# To Run .hql files from hive ::(.hql files you can write sql commands)

hive -f /home/<File_path and name>.hql
hive -f /home/<File_path and name>.hql


============================================================= Table creation SQL

-- Create temp table
CREATE EXTERNAL TABLE IF NOT EXISTS ${hiveconf:JOB_NAME}_temp ( 
  col_1 STRING,
  FILENAME STRING,
  col_2 STRING, 
  col_3 STRING
) 
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '|' 
STORED AS TEXTFILE 
LOCATION '${hiveconf:HDFSLocation}_temp';

=======================================================================
-- Create Final external base table 
CREATE EXTERNAL TABLE IF NOT EXISTS TableName_${hiveconf:JOB_NAME}_BASE_T ( 
 col_1 STRING,
 FILENAME STRING,
 col_2 STRING, 
 col_3 STRING
  
) 
PARTITIONED BY (col_2 STRING) 
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '|' 
STORED AS ORC 
LOCATION '${hiveconf:HDFSLocation}';


-- TODO : Replace INTO instead of OVERWRITE once the master table partition implemented
-- Insert partition data into final base table
INSERT OVERWRITE TABLE TableName_${hiveconf:JOB_NAME}_BASE_T PARTITION (OCC_DATE) SELECT OCC_TIMESTAMP,FILENAME,ESN,CALIBRATION_SOFTWARE_PHASE,ECM_CODE,ECM_NAME,ENGINE_MODEL,VIN,PCID,EVENT_ID,ECM_TIME,ECM_REAL_TIME,START_SOOT_FILL_MONITOR_STATUS,END_SOOT_FILL_MONITOR_STATUS,START_DOC_INLET_TEMPERATURE_UNIT,START_DOC_INLET_TEMPERATURE,MAXIMUM_DOC_OUT_TEMPERATURE_DURING_UNIT,MAXIMUM_DOC_OUT_TEMPERATURE_DURING,MAXIMUM_DPF_OUT_TEMPERATURE_DURING_UNIT,MAXIMUM_DPF_OUT_TEMPERATURE_DURING,MAXIMUM_DPF_DELTA_P_DURING_UNIT,MAXIMUM_DPF_DELTA_P_DURING,OCC_DATE FROM ${hiveconf:JOB_NAME}_TEMP DISTRIBUTE BY OCC_DATE SORT BY OCC_DATE;

-- Final View Creation
CREATE VIEW IF NOT EXISTS TableName_${hiveconf:JOB_NAME}_V AS SELECT * FROM MDSG_INSITE_${hiveconf:JOB_NAME}_BASE_T;

-- DROP tables
DROP TABLE IF EXISTS ${hiveconf:JOB_NAME}_transient;



=====================================================================================================================================

======================================================= Using Scoop

TIME_STAMP=`date +%Y-%m-%d-%H:%M:%S`
mkdir MES_ASSEMBLY_TESTDB_$TIME_STAMP
cd MES_ASSEMBLY_TESTDB_$TIME_STAMP

sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:oracle:thin:@jepxsspapp004.ced.corp.****** --username MCKI**** --password READ**** --query "SELECT * from Table where col_2 like '79%' AND \$CONDITIONS" --hive-database quality_mes --hive-table DISCO_ASSEMBLY --hive-import --target-dir /hive/warehouse/quality_mes.db/DISCO_ASSEMBLY_T1 --hive-delims-replacement '-'  -m 1  >> extract_DISCO_Logs.log 


sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:oracle:thin:@143.222.192.***** --username MCKI**** --password READ**** --query "SELECT NVL(vdpm.PART_DESC,'Unknown Desc') PART_DESC, col_3 from table_2 WHERE ( ( DECODE(vdda.ADD_PART_NO,NULL,vdda.OMIT_PART_NO,vdda.ADD_PART_NO) = LTRIM(SUBSTR(vdpm.PART_NO(+),1,7),'0')||RTRIM(SUBSTR(vdpm.PART_NO(+),8,2),'0') ) ) AND vdda.SERIAL_NO = twh.serial_no and twh.config_no IN ('D103016BX03', 'D103012BX03')  AND \$CONDITIONS" --hive-database quality_mes --hive-table DISCO_DEVIATION --hive-import  --target-dir /hive/warehouse/quality_mes.db/DISCO_DEVIATION --hive-delims-replacement '-' -m 1  >> extract_DISCO_DEVIATION_1-JAN-2013_1-JAN-2018.log

=============================================##Run below commands for pushing data to sql server for reporting

### Issue driver reporting data push with append

spark-submit --files $(for x in `ls -1 /home/sshuser/data_transformer/*.properties`; do readlink -f $x; done | paste -s | sed -e 's/\t/,/g') --class com.****.trans.driver.Driver --master yarn --deploy-mode cluster --queue default --name OMTransformer /home/sshuser/data_transformer/data-transformer-0.0.1-SNAPSHOT.jar action=omtrans process_mode=incremental_append source_type=hive source_db=quality_uat_reporting source_schema=quality_uat_reporting source_tablename=ISSUE_DRIVER_ISSUE_DETAIL target_type=mssql target_db=quantumb target_schema=reports target_tablename=ISSUE_DRIVER_ISSUE_DETAIL_WEEKLY target_db_url="jdbc:sqlserver://*****.database.windows.net:1433;database=quantumb" target_db_user=quantumadmin target_db_jeck="/code/data_ingestion/passwords/qb.password.jceks" target_db_alias=qb.password.alias hdfs_fs=wasbs://*****@quantumb.blob.core.windows.net  source_cols="A.*" numPartitions=400 update_col=run_analytics_date
spark-submit --files $(for x in `ls -1 /home/sshuser/data_transformer/*.properties`; do readlink -f $x; done | paste -s | sed -e 's/\t/,/g') --class com.****.trans.driver.Driver --master yarn --deploy-mode cluster --queue default --name OMTransformer /home/sshuser/data_transformer/data-transformer-0.0.1-SNAPSHOT.jar action=omtrans process_mode=incremental_append source_type=hive source_db=quality_uat_reporting source_schema=quality_uat_reporting source_tablename=ISSUE_DRIVER_UNIVERSAL_VIEW target_type=mssql target_db=quantumb target_schema=reports target_tablename=ISSUE_DRIVER_UNIVERSAL_VIEW_WEEKLY target_db_url="jdbc:sqlserver://*****.database.windows.net:1433;database=quantumb" target_db_user=quantumadmin target_db_jeck="/code/data_ingestion/passwords/qb.password.jceks" target_db_alias=qb.password.alias hdfs_fs=wasbs://*****@quantumb.blob.core.windows.net  source_cols="A.*" numPartitions=400 update_col=run_analytics_date

## OM reporting tables data push to sql server


spark-submit --files $(for x in `ls -1 /home/sshuser/data_transformer/*.properties`; do readlink -f $x; done | paste -s | sed -e 's/\t/,/g') --class com.cummins.trans.driver.Driver --master yarn --deploy-mode cluster --queue default --name OMTransformer /home/sshuser/data_transformer/data-transformer-0.0.1-SNAPSHOT.jar action=omtrans process_mode=overwrite source_type=hive source_db=quality_uat_reporting source_schema=quality_uat_reporting source_tablename=OM_ESN target_type=mssql target_db=quantumb target_schema=reports target_tablename=OM_ESN target_db_url="jdbc:sqlserver://cdsalphasqldb.database.windows.net:1433;database=quantumb" target_db_user=quantumadmin target_db_jeck="/code/data_ingestion/passwords/qb.password.jceks" target_db_alias=qb.password.alias hdfs_fs=wasbs://quantumbrserver@quantumb.blob.core.windows.net  source_cols="A.*" numPartitions=400
spark-submit --files $(for x in `ls -1 /home/sshuser/data_transformer/*.properties`; do readlink -f $x; done | paste -s | sed -e 's/\t/,/g') --class com.cummins.trans.driver.Driver --master yarn --deploy-mode cluster --queue default --name OMTransformer /home/sshuser/data_transformer/data-transformer-0.0.1-SNAPSHOT.jar action=omtrans process_mode=overwrite source_type=hive source_db=quality_uat_reporting source_schema=quality_uat_reporting source_tablename=OM_STAT target_type=mssql target_db=quantumb target_schema=reports target_tablename=OM_STAT_WEEKLY target_db_url="jdbc:sqlserver://cdsalphasqldb.database.windows.net:1433;database=quantumb" target_db_user=quantumadmin target_db_jeck="/code/data_ingestion/passwords/qb.password.jceks" target_db_alias=qb.password.alias hdfs_fs=wasbs://quantumbrserver@quantumb.blob.core.windows.net  source_cols="A.*" numPartitions=400


sqoop import -Dhive.metastore.uris=thrift://hn0-quantu.*****.dx.internal.cloudapp.net:9083 --connect jdbc:oracle:thin:@*****.aciz.cummins.com:1527:ebup1 --username MSBI_RELIABILITY --password dkabc3!  --table database.table_name --hive-database quality_reliability --hive-table table_name --hive-import --split-by columns_name --target-dir /hive/warehouse/*****.db/RLD_CLAIM_DETAIL --hive-delims-replacement '-' &> extract_sqoop.log

sqoop import -Dhive.metastore.uris=thrift://hn0-quantu.*****.dx.internal.cloudapp.net:9083 --connect jdbc:oracle:thin:@*****.aciz.cummins.com:1527:ebup1 --username MSBI_RELIABILITY --password dkabc3!  --table database.table_name --hive-database quality_reliability --hive-table table_name --hive-import --split-by columns_name --target-dir /hive/warehouse/*****.db/RLD_ENGINE_CLAIM_DETAIL_MV --hive-delims-replacement '-' &> extract_sqoop.log


======================================================================================================================-=============

=====================================================================================================================================

=====================================================================================================================================
