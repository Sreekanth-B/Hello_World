
======================================================================================================================================
======================================================================================================================================

===== RDD Operations in SPark 

// Spark - Core Programming 

// Reading the file from HDFS into RDD

val rdd =sc.textFile("HDFS/ Path");

// In the above snippet, sc refers to built-in SparkContext

//Filtering Message and Message Type fields
//For every record split the fields and concatenate Message Type(r(8)) and Message (r(9)) fields with “,”

val rdd2 = map(_.split("\t")).map(r => (r(8)+ ","+r(9)))


//Filtering the ERROR records and count the router specific errors

//Filter records starting with errors
val rdd3 = rdd2. filter(_.startsWith(“ERROR”)) 

//Count Rtr1 records
val R1Errs = rdd3.map(_.contains(“RTR1”)).count() 

// Count Rtr2 records
val R2Errs = rdd3.map(_.contains(“RTR2”)).count() 

// load the router error counts into array and paralleize into RDD
val outarray = Array(“Router 1 contains ”+R1Errs,”Router 2 contains ”+R2Errs); 

val outrdd = sc.paralleize(outarray)


// print and save final RDD into output file

outrdd.collect.foreach(println)

outrdd.saveAsTextFile("HDFS path to save")



//////////////// spliting the fields with "," and fetching only first 2 fields

val FieldsRDD = logsRDD.map(_.split(",")).map(r => (r(0),r(1)))


============================================================================================================================================

///// In this demo, Load the data as RDD, compute and find the number of unique occurrence of each record in the dataset
// Type in spark-shell command to open the Spark scala shell.
// Type in the below code line by line in the shell        

 val lines = sc.textFile(“/hdfspath/routerLog.tsv ")
 val pairs = lines.map(record => (record, 1))
 val counts = pairs.reduceByKey((a, b) => a + b)
 counts.collect.foreach(println)
 counts.collect.saveAsTextFile(“/hdfspath/myuniquedirectory”)

// You can find the output as (record, number of occurrences)


Consider a banking scenario where credit card transaction logs need to be processed. Log contains CustomerID, CustomerName, CreditCard Number, TransactionAmount. Which of the below code snippet creates a paired RDD <CustomerID, TransactionAmount>?
Ans :: val logsRDD = sc.textFile("/HDFSPath/Logs.txt"); val LogsPairedRDD = logsRDD.map(_.split(",")).map(r => (r(0),r(3).toInt))

============================================================================================================================================


// joins in spark data frames

// Implementation steps to join

// Step 1: Create Case classes representing datasets
// Create two case classes representing schema of each dataset.
// Case class representing RouterLocationInfo.tsv schema

case class RouterLocation(rid:Int,name:String,location:String);

// Case class representing RouterPurchaseInfo.tsv schema

case class RouterPurchase(rid:Int,date:String,pmemory:Long,smemory:Long,cost:Float);

// Step 2: Generate K,V pairs using case class object
// In this step,datasets are loaded as RDDs
// Paired RDDs  (K, V) are created where K = common column in both RDDs, V = Case class object.
//Load RouterLocation dataset and generate Rid(common field),RouterLocation object

val locRDD = sc.textFile(“RouterLocationInfo.tsv").map(_.split("\t")).map(r => (r(0), RouterLocation(r(0).toInt,r(1),r(2)))

//Load RouterPurchase dataset and generate Rid(common field),RouterLocation object

val purRDD = sc.textFile(“RouterPurchaseInfo.tsv").map(_.split("\t")).map(r => (r(0), RouterPurchase(r(0).toInt,r(1),r(2).toLong,r(3).toLong,r(4).toFloat))

// Step 3: Apply join() function
// In this step, Spark join is applied against the grouped fields of locRDD and purRDD from the previous step.
//Join locRDD with purRDD using join()

locRDD.join(purRDD).collect()


DEMO Steps:

      scala>case class RouterLocation(rid:Int,name:String,location:String);
       scala>case class RouterPurchase(rid:Int,date:String,pmemory:Long,smemory:Long,cost:Float);
       scala>val locdata = sc.textFile(“RouterLocationInfo.tsv").map(_.split("\t"));
       scala>val locRDD = locadata.map(r => (r(0), RouterLocation(r(0).toInt,r(1),r(2)));
       scala>val purRDD = sc.textFile(“RouterPurchaseInfo.tsv").map(_.split("\t")).map(r => (r(0),   RouterPurchase(r(0).toInt, r(1), r(2).toLong, r(3).toLong, r(4).toFloat));
       scala>locRDD.join(purRDD).collect()





============================================================================================================================================

////// Example : Below code is used to identify and count empty lines in dataset using Accumulator.

val rdd = sc.textFile("/HDFSPath");
//Create and initialize accumulator to count blank lines in  the file.

val blankLinesCounter = sc.accumulator(0);
//verify each line and increment the counter

rdd.foreach { line =>
if(line.length() == 0) blankLinesCounter  += 1
}
println("Empty Lines: " + blankLinesCounter )

============================================================================================================================================
============================================================================================================================================
========== RDD Operations From Net
// RDD operations in Spark Core

===========================================================================================================================
// reading data into rdd
val textFile = sc.textFile("hdfs://localhost: path")

// mapping each row as cols seperated with tab
val map_test = textFile.map(line => line.split("\t"))

// printing the above rdd
map_text.collect

// selecting 2,3 col values
val =map_text1 = map_text.map(line => (line(1),line(2)))
 

// filtering wheather the col 3 having india as value

val fil = map_text.filter(line => line(2).contains("india"))

fil.collect

===========================================================================================================================

// getting count of medals own by each unique country

val map_text2 = map_text.map(line =>line(2),line(9).toInt))

map_text2.reduceByKey(_+_).collect
 

// Here in this scenario, we have taken a pair of Country and total medals columns as key and value and we are performing reduceByKey operation on the RDD.

// We have got the final result as country and the total number of medals won by each country in all the Olympic games.

// count is used to return number of elements in the rdd

// selecting first and third col values
val map_text2 = map_text.map(line => line(0),line(2)))

map_text2.count
//you can see that there are 8618 records in the RDD map_test1.

===========================================================================================================================

// CountByValue: countByValue is used to count the number of occurrences of the elements in the RDD. Below is the sample demonstration of the above scenario.

val map.text1 =map_text.map(line => line(2),line(5)))
map_text1.countByValue

// In the above scenario, we have taken a pair of Country and Sport. By performing countByValue action we have got the count of each country in a particular sport.
===========================================================================================================================

// Reduce: Below is the sample demonstration of the above scenario where we have taken the total number of medals column in the dataset and loaded into the RDD map_test1. On this RDD we are performing reduce operation. Finally, we have got that there is a total of 9529 medals declared as the winners in Olympic.

val map_text3 = map_text.map(line => line(9).toInt))
map_text3.reduce((a,b) => a+b)
>> res: Int =9529

===========================================================================================================================
// Take: take will display the number of records we explicitly specify. Below is the sample demonstration of the above scenario.

val map_text4 = map_text.map(line => line(2),line(9).toInt))
map.text4.reduceByKey(_+_).collect
// above command gives all the records in the rdd, to print only first 2 records we use below command
map_text4.reduceByKey(_+_).take(2)

===========================================================================================================================
===========================================================================================================================

// ========= Advance RDD Operations
// Here, we have taken two datasets, dept and emp, 

// RDD’s holding Objects: Here, by using the case class, we will declare one object and will pass this case class as parameter to the RDD. You can refer to the below screen shot for the same.

val dpt = sc.textFile(“hdfs://local: path”)
case class dept(id:String,name:String)
val split_dpt = dpt.map(line=> line.split(“\t”)).map(line => (dept(line(0),line(1))))
split_dpt.collect

===========================================================================================================================

// Foreach: The foreach operation is used to iterate every element in the spark RDD. You can refer to the below screen shot for the same.

emp.collect.foreach(println)
 
// you can see that every element in the spark RDD emp are printed in a separate line.

// CountByKey: The CountByKEy operation returns the number of elements present for each key. You can refer to the below screenshot for the same.

Val dpt = sc.textFile(“hdfs://local:path”)
Val split_dpt =dpt.map(line => line.split(“\t”)).map(line => (line(0).toString, line(1).toString))
Split_dpt.countByKey

// Here, we have loaded the dataset and split the records by using tab as delimiter and created the pair as DeptNo and DeptName. Then, we have performed CountByKey operation and the result is as displayed.
===========================================================================================================================

// SaveAsTextFile: The SaveAsTExtFile operation stores the result of the RDD in a text File in the given output path. You can refer to the below screenshot for the same.

split_dpt.saveAsTextFile(“hdfs://local: path”)

// Keys: The Keys operation is used to print all the keys in the RDD pair. You can refer to the below screen shot for the same.

Val emp = sc.textFile(“hdfs”://local)
Import org.apache.spark.rdd.RDD
Val pairs:RDD[String, Int)] = emp.map(line => line.split(“\t”)).map(line => (line(2).toString,line(0).toInt))
pairs.keys.collect
 
// Values: The Values operation is used to print all the values in the RDD pair. You can refer to the below screen shot for the same.

Val pairs:RDD[String, Int)] = emp.map(line => line.split(“\t”)).map(line => (line(2).toString,line(0).toInt))
pairs.values.collect
 
===========================================================================================================================

// SortByKey:The SortByKey operation returns the RDD that contains the key value pairs sorted by Keys. SortByKey accepts arguments true/false. ‘False’ will sort the keys in descending order and ‘True’ will sort the keys in ascending order. You can refer to the below screen shot for the same.l

Val pairs:RDD[String, Int)] = emp.map(line => line.split(“\t”)).map(line => (line(2).toString,line(0).toInt))
pairs.sortByKey(false).collect
pairs.sortByKey(true).collect

===========================================================================================================================

// Union: The Union operation results in an RDD which contains the elements of both the RDD’s. You can refer to the below screen shot to see how the Union operation performs.

val dpt = sc.textFile(“hdfs://local: path”)
val emp = sc.textFile(“hdfs://local: path”)
val test = dpt.union(emp)
test.take(10).foreach(println)
 
// Here, we have created two RDDs and loaded the two datasets into them. We have performed Union operation on them, and from the result, you can see that both the datasets are combined and have printed the first 10 records of the newly obtained spark RDD. Here the 10th record is the first record of the second dataset.

// Intersection: Intersection returns the elements of both the RDD’s. Refer the below screen shot to know how to perform intersection.

Val split_dpt = dpt.map(line => line.split(“\t”)).map(line =>line(0) )
Val split_emp = emp.map(line => line.split(“\t”)).map(line =>line(6))

val test = split_dpt.intersection(split_emp)
test.collect
// Here we have split the datasets by using tab delimiter and have extracted the 1st column from the first dataset and the 7th column from the second dataset. We have also performed intersection on the datasets and the result is as displayed.

===========================================================================================================================

// Cartesian: The Cartesian operation will return the RDD containing the Cartesian product of the elements contained in both the RDDs. You can refer to the below screen shot for the same.
val test = split_dpt.cartesian(split_emp)
test.collect
 
// Here we have split the datasets by using tab delimiter and have extracted 1st column from the first dataset and 7th column from the second dataset. Then, we have performed the Cartesian operation on the RDDs and the results are displayed.




============================================================================================================================================
// Spark from Reference Documents  for RDD Operations


// opening the spark concole
spark-shell

// to know the installed spark version
spark.version

// filtering the odd values in the array
// creaating the array
val ages = Array(20,50,35,41)

// priting the array values
ages.foreach(println)

// creating the function object for odd value logic
def isOdd(age:Int) : Boolean = {
(age % 2) == 1
}

// using the function object on array and printing the odd values in the array
ages.filter(age => isOdd(age)).foreach(println)

============================================================================================================================================


// Creating an RDD from a File Data Source
val fileRDD = spark.sparkContext.textFile("/tmp/data.txt")

// Filtering for Lines That Contain the Word Awesome
val awesomeLineRDD = stringRDD.filter(line => line.contains("awesome"))
awesomeLineRDD.collect.foreach(println)

// Defining a function to perform addition
def add(v1:Int, v2:Int) : Int = {
println(s"v1: $v1, v2: $v2 => (${v1 + v2})")
v1 + v2
}

============================================================================================================================================


//////////////////////  

//Loading a text file in to an RDD
val Car_Info = sc.textFile("/HDFSPath/ArisconnCars.txt");

//Referring the header of the file
val header = Car_Info.first();

//Removing header and splitting records with ',' as delimiter and fetching relevant fields
val Car_temp = Car_Info.filter(record => record!=header).map(_.split(",")).map(c =>(c(0),c(1),c(2).toDouble,c(3).toDouble,c(6).toInt,c(9)));

//Filtering only valid records(records not starting with '?'), and _._1 refers to first field (sensorid)
val Car_Eng_Specs = Car_temp.filter(!_._1.startsWith("?"));

//Filtering records holding only error messages and _._6 refers to 6th field (Typeofmessage)

val Car_Error_logs = Car_Eng_Specs.filter(_._6.startsWith("ERR"));



============================================================================================================================================

Code to compute the number of errors produced by every car
//Loading a text file in to an RDD
val Car_Info = sc.textFile("/HDFSPath/ArisconnCars.txt");
//Splitting records with ',' as delimitter and fetching relevant fields
val Car_temp = Car_Info.map(_.split(",")).map(c =>(c(0), c(1), c(2).toDouble, c(3).toDouble, c(6).toInt, c(9)));
// Filtering only valid records not starting with '?'
val Car_Eng_Specs = Car_temp.filter(!_._1.startsWith("?"));
val Car_Error_logs = Car_Eng_Specs.filter(_._5.startsWith("ERR"));
//Filtering records holding only error messages
val Car1Errors = Car_Error_logs.filter(_._1.startsWith("CAR_34853")).count();
//Filtering car1 records and counting the number of occurences
val Car2Errors = Car_Error_logs.filter(_._1.startsWith("CAR_34854")).count();
//Filtering car2 records and counting the number of occurences 
.


//Lines of code would increase as per more number of cars.
//Combining and aggregating all the cars errors and printing them would be highly challenging
We observed that count() in the above code is applied individually on each car. With more number of cars, more number of statements would be required.
Same applies to other complex aggregate functions such as AVG, SUM, MIN, MAX. 



============================================================================================================================================
============================================================================================================================================
============================================================================================================================================

============================================================================================================================================

Spark-SQL Code

//SQL Context object creation in Spark SQL
val sqlContext = new org.apache.spark.sql.SQLContext(sc);

//Loading a text file in to an RDD
val Car_Info = sc.textFile("hdfs://vimsmys-42:9000/user/jai_trng/jaihdfs/ArisconnCars.txt");

// Creating a case class mapping the fields in the dataset
case class Cars(sensorid:String, carid:String, latitude:Double, longitude:Double, engine_speed:Int, accelerator_pedal_position:Int, vehicle_speed:Int, torque_at_transmission:Int, fuel_level:Double, TypeOfMessage:String, timestamp:Double);
val header = Car_Info.first();

// Creating a Spark SQL DataFrame
val DF = Car_Info.filter(c => c!=header).map(_.split(",")).map(c => Cars(c(0), c(1), c(2).toDouble, c(3).toDouble, c(4).toInt, c(5).toInt, c(6).toInt, c(7).toInt, c(8).toDouble, c(9), c(10).toDouble)).toDF();

//Registering the DataFrame as a temporary table
DF.registerTempTable("cars");

train.registerAsTable('train_table')

// A simple query to implement all 4 requirements of Arisconn cars
val errors= sqlContext.sql("SELECT sensorid,carid,latitude,longitude,vehicle_speed,count(TypeOfMessage) FROM cars WHERE sensorID!='sensorID' AND carid!='?' AND TypeOfMessage Like 'ERR.*' group by carid");



============================================================================================================================================

// Requirement : ArisconnCars dataset has few corrupted records which needs to be filtered.

// Solution:

// 2. Create a SQLContext object using SQLContext class
//Creation of a SQLContext object in Spark SQL

val sqlContext = new org.apache.spark.sql.SQLContext(sc);

// 1. Create a SparkContext object and load the dataset using the SparkContext object's textFile() method
// File provided in the path is loaded in to Spark RDD using SparkContext object 'sc'

val Car_Info = sc.textFile("/HDFSpath/ArisconnCars.txt");

//dataset's first record contains schema details which needs to be ignored while processing

val header = Car_Info.first()

// 3. Create a DataFrame and load a normal RDD in to the DataFrame using Case class
// Creation of a case class mapping the fields in the dataset

case class Cars(sensorid: String, carid: String, latitude:Double, longitude:Double, engine_speed:Int, accelerator_pedal_position:Int, vehicle_speed:Int, torque_at_transmission:Int, fuel_level:Double, TypeOfMessage:String, timestamp:Double)

/*Creation of a Spark SQL DataFrame by delimiting the fields, and loading them as case class properties. Note: "toInt", "toDouble" are Scala methods for converting text fields in to numerical*/

val DF = Car_Info.filter(c => c!=header).map(_.split(",")).map(c => Cars(c(0), c(1), c(2).toDouble, c(3).toDouble, c(4).toInt, c(5).toInt, c(6).toInt, c(7).toInt, c(8).toDouble, c(9), c(10).toDouble)).toDF()

// 4. Register the DataFrame as a normal table using registerTempTable() method
//Registering the DataFrame as a temporary table

DF.registerTempTable("cars");

// 5. Write and execute Spark SQL queries against the table using SQLContext object's sql() method
/* Query using sql method to filter corrupted records and consider only valid data for further analysis */

val valrecords= sqlContext.sql("SELECT sensorid,carid,latitude,longitude,vehicle_speed,TypeOfMessage FROM cars WHERE sensorid Like 'SEN_%' AND carid Like 'CAR_%' AND sensorid!='sensorID'");

//display the dataframe records on console

valrecords.show()

============================================================================================================================================

// Working with Data Sets

1.	val sqlContext = new org.apache.spark.sql.SQLContext(sc);
2.	
3.	val Cust_Info = sc.textFile("/HDFSPath/Customer.txt");
4.	
5.	case class Customer(custid: String,firstname: String,lastname:String,age:Int,profession:String);
6.	
7.	//Creating a Dataset using toDS() method
8.	
9.	val DS = Cust_Info.map(_.split(",")).map(c => Customer(c(0),c(1),c(2),c(3).toInt,c(4)).toDS();
10.	
11.	//Creating and registering the Dataset as a temporary view
12.	
13.	DS.createOrReplaceTempView("customer");
14.	
15.	//Spark SQL query to compute Profession, Average age of that profession
16.	
17.	val pilotavg= sqlContext.sql("SELECT profession,AVG(age) FROM customer group by profession");
18.	
19.	pilotavg.show();



============================================================================================================================================
============================================================================================================================================
// Working with Parquet Files in Spark Core

1. Load/Structure the Dataset
1.	//SQL Context object creation in Spark SQL
2.	
3.	val sqlContext = new org.apache.spark.sql.SQLContext(sc);
4.	
5.	//File in args(0) is loaded in to Spark RDD using SparkContext object 'sc'
6.	
7.	val Car_Info = sc.textFile("/HDFSPath/ArisconnCars.txt");
8.	
9.	//First record in dataset is the list of column headers which need to be ignored while processing.
10.	
11.	val header = Car_Info.first() 
2. Create a Case class to map the fields of the dataset
1.	// Creating a case class mapping the fields in the dataset
2.	
3.	case class Cars(sensorid:String, carid:String, latitude:Double, longitude:Double, engine_speed:Int, accelerator_pedal_position:Int, vehicle_speed:Int, torque_at_transmission:Int, fuel_level:Int, TypeOfMessage:String, timestamp:Double) 
 
3. Create a DataFrame and register as temporary table
1.	// Creating a Spark SQL DataFrame by delimiting the fields and loading them as Case class properties. Note that, "toInt", "toDouble" are Scala methods for converting text fields in to numericals
2.	
3.	val DF = Car_Info.filter(c => c!=header).map(_.split(",")).map(c => Cars(c(0), c(1), c(2).toDouble, c(3).toDouble, c(4).toInt, c(5).toInt, c(6).toInt, c(7).toInt, c(8).toDouble, c(9), c(10).toDouble)).toDF();
4.	
5.	//Registering the DataFrame as a temporary table
6.	
7.	DF.registerTempTable("cars");
8.	
9.	// Write Spark SQL query to filter valid records
10.	
11.	//Spark SQL Query to fetch only valid records where sensorid starts with SEN and carid starts with CAR
12.	
13.	val valrecords= sqlContext.sql("SELECT sensorid,carid,latitude,longitude,vehicle_speed,TypeOfMessage FROM cars WHERE sensorid Like 'SEN_%' AND carid Like 'CAR_%'")
14.	
15.	 
4. Store as Parquet
1.	//Store the results of the DataFrame as a parquet file in an HDFS directory
2.	
3.	valrecords.write.parquet("/HDFSPath/CarsParquetData") 
 
5. Create a DataFrame from parquet file and view
1.	//read data from parquet file and create a DataFrame
2.	
3.	val CarsDF = sqlContext.read.parquet("/HDFSPath/CarsParquetData")
4.	
5.	//display the DataFrame content
6.	
7.	CarsDF.show()


============================================================================================================================================

// Working with JSON files in Spark Core
// for JSON files schema will be inferred automatically based on sample values


1.	//Creation of SQLContext
2.	
3.	object val sqlContext = new org.apache.spark.sql.SQLContext(sc);
4.	
5.	//Spark SQL provides read.json() method to load JSON files as DataFrames directly
6.	
7.	val cars = sqlContext.read.json("/HDFSPath/Cars.json");
8.	
9.	//Schema of JSON files shall be printed using printSchema() method
10.	
11.	cars.printSchema();
12.	
13.	//Creation of a temporary table
14.	
15.	cars.registerTempTable("carstable");
16.	
17.	//Spark SQL query against carstable
18.	
19.	val op = sqlContext.sql("SELECT * from carstable");
20.	
21.	//show() method to print output
22.	
23.	op.show(); 




========================================================================================================================================================================================================================================================================================
// Working with Avro file 

1.	import com.databricks.spark.avro._
2.	
3.	//Creation of SQLContext object
4.	
5.	val sqlContext = new org.apache.spark.sql.SQLContext(sc);
6.	
7.	//Spark SQL provides read.avro() method to load Avro files as DataFrames directly
8.	
9.	val cars = sqlContext.read.avro("/HDFS/Cars.avro");
10.	
11.	//Schema of avro files shall be printed using printSchema() method
12.	
13.	cars.printSchema();
14.	
15.	//Creation of a temporary table
16.	
17.	cars.registerTempTable("carstable");
18.	
19.	//Spark SQL query against carstable
20.	
21.	val op = sqlContext.sql("SELECT * from carstable");
22.	
23.	//show() method to print output
24.	
25.	op.show();



// Which of the below method is used to save a DataFrame "dfParquet" as a parquet file in HDFS?

dfParquet.write.parquet("/HDFS destination folder path")

// Which of the below method is used to read an AVRO file and create DataFrame?

SQLContext.read.avro("HDFSPath")

============================================================================================================================================
======================================== Spark - SQL for Hive tables

/* Load Parquet File as DataFrame
// In this step, create a SQLContext object and use read.parquet() method to load the parquet file created in our earlier requirement */

val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val carsparq = sqlContext.read.parquet("/HDFS/CarsParq.parquet") 
 
/* Save DataFrame into Hive Table
// In this step, write.option() is used to specify the Hive metastore directory for the Hive table. 
 // saveAsTable() is used to persist the DataFrame as a hive table.
// write.option method use to specify the path for the Hive table and saveAsTable method creates a Hive table in Hive metastore */
	
carsparq.write.option("path",'/HDFS/hive_tables_location/jaicarsparq').saveAsTable("SparkHiveCarsTable") 
 
// 3. Spark SQL queries on Hive Table
/* In this step :
 //     HiveContext object is created and using which Spark SQL queries are written against the existing Hive tables.
//      HiveContext object provides an entry point for Spark to Hive.
//	Creation of HiveContext object for Spark to connect to Hive */
	
val hContext = new org.apache.spark.sql.hive.HiveContext(sc);
	
//Spark SQL query on a hive table to compute the top 10 vehicles which were running on maximum speed
	
val CarHiveData = hContext.sql("select carid,latitude,longitude,MAX(vehicle_speed) from SparkHiveCarsTable GROUP BY carid limit 10")
	
//Output of the query to be displayed
	
CarHiveData.show();



//// TO see the hive table in hive concole

hive > show tables 'SparkHiveCarsTable';

// above code will give table if present. we can write direct hive queries on top of hive tables

======================================================================================================================================
=============== Reading and Wrriting the files for different formats

==================== Parquet File operation

// Load/Structure the Dataset
//SQL Context object creation in Spark SQL
	
val sqlContext = new org.apache.spark.sql.SQLContext(sc);
	
//File in args(0) is loaded in to Spark RDD using SparkContext object 'sc'

val Car_Info = sc.textFile("/HDFSPath/ArisconnCars.txt");
	
//First record in dataset is the list of column headers which need to be ignored while processing.
	
val header = Car_Info.first() 
//Create a Case class to map the fields of the dataset
// Creating a case class mapping the fields in the dataset

case class Cars(sensorid:String, carid:String, latitude:Double, longitude:Double, engine_speed:Int, accelerator_pedal_position:Int, vehicle_speed:Int, torque_at_transmission:Int, fuel_level:Int, TypeOfMessage:String, timestamp:Double) 
 
// Create a DataFrame and register as temporary table
// Creating a Spark SQL DataFrame by delimiting the fields and loading them as Case class properties. Note that, "toInt", "toDouble" are Scala methods for converting text fields in to numericals
	
val DF = Car_Info.filter(c => c!=header).map(_.split(",")).map(c => Cars(c(0), c(1), c(2).toDouble, c(3).toDouble, c(4).toInt, c(5).toInt, c(6).toInt, c(7).toInt, c(8).toDouble, c(9), c(10).toDouble)).toDF();

//Registering the DataFrame as a temporary table

DF.registerTempTable("cars");
	
// Write Spark SQL query to filter valid records
	
//Spark SQL Query to fetch only valid records where sensorid starts with SEN and carid starts with CAR

val valrecords= sqlContext.sql("SELECT sensorid,carid,latitude,longitude,vehicle_speed,TypeOfMessage FROM cars WHERE sensorid Like 'SEN_%' AND carid Like 'CAR_%'")

================
	 
//Store as Parquet
//Store the results of the DataFrame as a parquet file in an HDFS directory
	
valrecords.write.parquet("/HDFSPath/CarsParquetData") 

// Create a DataFrame from parquet file and view
//read data from parquet file and create a DataFrame
	
val CarsDF = sqlContext.read.parquet("/HDFSPath/CarsParquetData")

//display the DataFrame content
	
CarsDF.show()


=============================== JSON FIle Operations
//Creation of SQLContext

object val sqlContext = new org.apache.spark.sql.SQLContext(sc);
	
//Spark SQL provides read.json() method to load JSON files as DataFrames directly
	
val cars = sqlContext.read.json("/HDFSPath/Cars.json");
	
//Schema of JSON files shall be printed using printSchema() method
	
cars.printSchema();
	
//Creation of a temporary table
	
cars.registerTempTable("carstable");
	
//Spark SQL query against carstable
	
val op = sqlContext.sql("SELECT * from carstable");

//show() method to print output

op.show();

// writting the op into Json

op.write.json("/HDFSPath/CarsParquetData") 


=============================== AVRO FIle Operations

import com.databricks.spark.avro._
	
//Creation of SQLContext object
	
val sqlContext = new org.apache.spark.sql.SQLContext(sc);
	
//Spark SQL provides read.avro() method to load Avro files as DataFrames directly
	
val cars = sqlContext.read.avro("/HDFS/Cars.avro");
	
//Schema of avro files shall be printed using printSchema() method
	
cars.printSchema();

//Creation of a temporary table

cars.registerTempTable("carstable");

//Spark SQL query against carstable
	
val op = sqlContext.sql("SELECT * from carstable");

//show() method to print output

op.show();

// Wrriting into AVRO file
op.write.avro("Path")




======================================================================================================================================
======================================================================================================================================
======================= For Document PDF

// Creating a DataFrame from an RDD of Numbers

import scala.util.Random
val rdd = spark.sparkContext.parallelize(1 to 10).map(x => (x,
Random.nextInt(100)* x))
val kvDF = rdd.toDF("key","value")

// Printing the Schema and Showing the Data of a DataFrame
kvDF.printSchema

kvDF.show

// Calling the Function show to Display Five Rows in Tabular Format
kvDF.show(5)

// Reading the README.md File As a Text File from a Spark Shell
val textFile = spark.read.text("README.md")
textFile.printSchema

// show 5 lines and don't truncate
textFile.show(5, false)


============================================================================================================================================
// Reading CSV Files with Various Options
val movies = spark.read.option("header","true").csv("<path>/book/chapter4/
data/movies/movies.csv")
movies.printSchema


// now try to manually provide a schema
import org.apache.spark.sql.types._
val movieSchema = StructType(Array(StructField("actor_name", StringType, true),
StructField("movie_title", StringType, true),
StructField("produced_year", LongType, true)))
val movies3 = spark.read.option("header","true").schema(movieSchema)
.csv("<path>/book/chapter4/data/movies/
movies.csv")
movies3.printSchema

//Reading a TSV File with the CSV Format
val movies4 = spark.read.option("header","true").option("sep", "\t")
.schema(movieSchema).csv("<path>/book/chapter4/data/movies/movies.tsv")
movies.printSchema



// Writing the Data to csv file

Common Interacting Pattern with DataFrameWriter

movies.write.format(...).mode(...).option(...).partitionBy(...).bucketBy(...)
.sortBy(...).save(path)



Using DataFrameWriter to Write Data to File-Based Sources
// write data out as CVS format, but using a '#' as delimiter

movies.write.format("csv").option("sep", "#").save("/tmp/output/csv")
// write data out using overwrite save mode
movies.write.format("csv").mode("overwrite").option("sep", "#").save
("/tmp/output/csv")


========================================================================================================================================================================================================================================================================================
// Various Examples of Reading a JSON File
val movies5 = spark.read.json("<path>/book/chapter4/data/movies/movies.json")
movies.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)


// specify a schema to override the Spark's inferring schema.
// producted_year is specified as integer type

import org.apache.spark.sql.types._
val movieSchema2 = StructType(Array(StructField("actor_name", StringType, true),
StructField("movie_title", StringType, true),
StructField("produced_year", IntegerType, true)))

val movies6 = spark.read.option("inferSchema","true").schema(movieSchema2)
.json("<path>/book/chapter4/data/movies/movies.json")
movies6.printSchema


============================================================================================================================================
// Reading a Parquet File in Spark
// Parquet is the default format, so we don't need to specify the format
when reading
val movies9 = spark.read.load("<path>/book/chapter4/data/movies/movies.
parquet")
movies9.printSchema

// If we want to be more explicit, we can specify the path to the parquet
function
val movies10 = spark.read.parquet("<path>/book/chapter4/data/movies/movies.
parquet")
movies10.printSchema

// Reading an ORC File in Spark
val movies11 = spark.read.orc("<path>/book/chapter4/data/movies/movies.orc")
movies11.printSchema
============================================================================================================================================

=============== SELECTING THE SPECIFIC COLUMNS

// to display column names in a DataFrame, we can call the columns function
kvDF.columns

kvDF.select("key")

kvDF.select('key)
// using the col function of DataFrame
kvDF.select('key, 'key > 1).show
+---+----------+
|key| (key > 1)|
+---+----------+
| 1| false|
| 2| true|
+---+----------+

// TO fetch only one field from the df and showing the results

df.select("name").show();

// Two Variations of the select Transformation
movies.select("movie_title","produced_year").show(5)

+-------------------+--------------+
| movie_title| produced_year|
+-------------------+--------------+
| Coach Carter| 2005|
| Superman II| 1980|



// using a column expression to transform year to decade
movies.select('movie_title,('produced_year - ('produced_year % 10)).
as("produced_decade")).show(5)
+-------------------+----------------+
| movie_title| produced_decade|
+-------------------+----------------+
| Coach Carter| 2000|
| Superman II| 1980|



Adding the decade Column to the movies DataFrame Using a SQL
Expression
movies.selectExpr("*","(produced_year - (produced_year % 10)) as decade").
show(5)
+-----------------+-------------------+--------------+-------+
| actor_name| movie_title| produced_year| decade|
+-----------------+-------------------+--------------+-------+
|McClure, Marc (I)| Coach Carter| 2005| 2000|



===================== FILTERING THE COLUMNS FOR SPECIFIED FILTERS

Filter Rows with Logical Comparison Functions in the Column Class
movies.filter('produced_year < 2000)
movies.where('produced_year > 2000)
movies.filter('produced_year >= 2000)
movies.where('produced_year >= 2000)

// equality comparison require 3 equal signs
movies.filter('produced_year === 2000).show(5)

// inequality comparison uses an interesting looking operator =!=
movies.select("movie_title","produced_year").filter('produced_year =!=
2000).show(5)

// to combine one or more comparison expressions, we will use either the OR
and AND expression operator
movies.filter('produced_year >= 2000 && length('movie_title) < 5).show(5)

// the other way of accomplishing the same result is by calling the filter
function two times
movies.filter('produced_year >= 2000).filter(length('movie_title) < 5).show(5)


=============

// filter data down to smaller size to make it easier to see the rollups result
val twoStatesSummary = flight_summary.select('origin_state, 'origin_city, 'count)
.where('origin_state === "CA"
|| 'origin_state === "NY")
.where('count > 1 && 'count < 20)
.where('origin_city =!= "White
Plains")
.where('origin_city =!=
"Newburgh")
.where('origin_city =!=
"Mammoth Lakes")
.where('origin_city =!=
"Ontario")


====================================== DISTINCT COUNT OF COLUMN VALUES

Using distinct and dropDuplicates to Achieve the Same Goal
movies.select("movie_title").distinct.selectExpr("count(movie_title) as
movies").show
movies.dropDuplicates("movie_title").selectExpr("count(movie_title) as
movies").show

Using a SQL Expression and Built-in Functions
movies.selectExpr("count(distinct(movie_title)) as
movies","count(distinct(actor_name)) as actors").show
+-------+-------+
| movies| actors|
+-------+-------+
| 1409| 6527|
+-------+-------+
============================================================================================================================================
========== sorting operation 

// Sorting the DataFrame in Ascending and Descending Order
// sorting in Ascending order
movieTitles.sort('title_length).show(5)

// sorting in descending order
movieTitles.orderBy('title_length.desc).show(5)

// sorting by two columns in different orders
movieTitles.orderBy('title_length.desc, 'produced_year).show(5)

============ limit operation

Using the limit Transformation to Figure Out the Top Ten Actors
with the Longest Names
// first create a DataFrame with their name and associated length
val actorNameDF = movies.select("actor_name").distinct.selectExpr
("*", "length(actor_name) as length")

// order names by length and retrieve the top 10
actorNameDF.orderBy('length.desc).limit(10).show

========================================================================================================================================================================================================================================================================================

//Adding a Missing Actor to the movies DataFrame
// we want to add a missing actor to movie with title as "12"
val shortNameMovieDF = movies.where('movie_title === "12")
shortNameMovieDF.show

// create a DataFrame with one row
import org.apache.spark.sql.Row
val forgottenActor = Seq(Row("Brychta, Edita", "12", 2007L))
val forgottenActorRDD = spark.sparkContext.parallelize(forgottenActor)
val forgottenActorDF = spark.createDataFrame(forgottenActorRDD,
shortNameMovieDF.schema)

// now adding the missing action
val completeShortNameMovieDF = shortNameMovieDF.union(forgottenActorDF)
completeShortNameMovieDF.union(forgottenActorDF).show
============================================================================================================================================

Adding a Column As Well As Replacing a Column Using the
withColumn Transformation
// adding a new column based on a certain column expression

movies.withColumn("decade", ('produced_year - 'produced_year % 10)).show(5)
+------------------+-------------------+--------------+-------+
| actor_name| movie_title| produced_year| decade|
+------------------+-------------------+--------------+-------+

// now replace the produced_year with new values
movies.withColumn("produced_year", ('produced_year - 'produced_year % 10)).
show(5)
+------------------+-------------------+--------------+
| actor_name| movie_title| produced_year|


// ========= withColumnRenamed(existingColName, newColName)

Using the withColumnRenamed Transformation to Rename Some
of the Column Names
movies.withColumnRenamed("actor_name", "actor")
.withColumnRenamed("movie_title", "title")
.withColumnRenamed("produced_year", "year").show(5)
+------------------+-------------------+-----+
| actor| title| year|

// Dropping Two Columns: One Exists and the Other One Doesn’t
movies.drop("actor_name", "me").printSchema

// As you can see from the previous example, the second column, me, doesn’t exist in
the schema, so the drop transformation simply ignores it.
============================================================================================================================================
Using randomSplit to split the movies DataFrames into Three Parts
// the weights need to be an Array
val smallerMovieDFs = movies.randomSplit(Array(0.6, 0.3, 0.1))
// let's see if the counts are added up to the count of movies DataFrame
movies.count
Long = 31393
smallerMovieDFs(0).count
Long = 18881
smallerMovieDFs(0).count + smallerMovieDFs(1).count + smallerMovieDFs(2).
count
Long = 31393

============================================================================================================================================

Dropping Rows with Missing Data
// first create a DataFrame with missing values in one or more columns
import org.apache.spark.sql.Row
val badMovies = Seq(Row(null, null, null),
Row(null, null, 2018L),
Row("John Doe", "Awesome Movie", null),
Row(null, "Awesome Movie", 2018L),
Row("Mary Jane", null, 2018L))
val badMoviesRDD = spark.sparkContext.parallelize(badMovies)
val badMoviesDF = spark.createDataFrame(badMoviesRDD, movies.schema)
badMoviesDF.show
+----------+--------------+--------------+
|actor_name| movie_title| produced_year|
+----------+--------------+--------------+
| null| null| null|
| null| null| 2018|
| John Doe| Awesome Movie| null|
| null| Awesome Movie| 2018|
| Mary Jane| null| 2018|
+----------+--------------+--------------+



// dropping rows that have missing data in any column
// both of the lines below will achieve the same purpose
badMoviesDF.na.drop().show
badMoviesDF.na.drop("any").show
+----------+------------+--------------+
|actor_name| movie_title| produced_year|
+----------+------------+--------------+
+----------+------------+--------------+
// drop rows that have missing data in every single column
badMoviesDF.na.drop("all").show
+----------+--------------+--------------+
|actor_name| movie_title| produced_year|
+----------+--------------+--------------+
| null| null| 2018|
| John Doe| Awesome Movie| null|
| null| Awesome Movie| 2018|
| Mary Jane| null| 2018|
+----------+--------------+--------------+
// drops rows when column actor_name has missing data
badMoviesDF.na.drop(Array("actor_name")).show
+----------+--------------+--------------+
|actor_name| movie_title| produced_year|
+----------+--------------+--------------+
| John Doe| Awesome Movie| null|
| Mary Jane| null| 2018|
+----------+--------------+--------------+




========================================================================================================================================================================================================================================================================================
Use the describe Transformation to Show Statistics for the
produced_year Column
movies.describe("produced_year").show
+-------+-------------------+
|summary| produced_year|
+-------+-------------------+
| count| 31392|
| mean| 2002.7964449541284|
| stddev| 6.377236851493877|
| min| 1961|
| max| 2012|
+-------+-------------------+
============================================================================================================================================


// now register movies DataFrame as a temporary view
movies.createOrReplaceTempView("movies")

Programmatically Executing SQL Statements in Spark
// simple example of executing a SQL statement without a registered view
val infoDF = spark.sql("select current_date() as today , 1 + 100 as value")
infoDF.show


// select from a view
spark.sql("select * from movies where actor_name like '%Jolie%' and
produced_year > 2009").show


// mixing SQL statement and DataFrame transformation
spark.sql("select actor_name, count(*) as count from movies group by actor_name")
.where('count > 30)
.orderBy('count.desc)
.show


============================================================================================================================================

Using DataFrameWriter to Write Data to File-Based Sources
// write data out as CVS format, but using a '#' as delimiter
movies.write.format("csv").option("sep", "#").save("/tmp/output/csv")
// write data out using overwrite save mode
movies.write.format("csv").mode("overwrite").option("sep", "#").save
("/tmp/output/csv")


============================================================================================================================================

Creating a DataFrame by Reading a Flight Summary Dataset
val flight_summary = spark.read.format("csv")
.option("header", "true")
.option("inferSchema","true")
.load("<path>/chapter5/data/flights/
flight-summary.
csv")
// use count action to find out number of rows in this data set
flight_summary.count()


Computing the Count for Different Columns in the flight_summary
DataFrame
flight_summary.select(count("origin_airport"), count("dest_airport").
as("dest_count")).show



// now performing the count aggregation on different columns
badMoviesDF.select(count("actor_name"), count("movie_title"),
count("produced_year"), count("*")).show
+------------------+-------------------+---------------------+---------+
| count(actor_name)| count(movie_title)| count(produced_year)| count(1)|
+------------------+-------------------+---------------------+---------+
| 2| 3| 4| 4|


Counting Unique Items in a Group
flight_summary.select(countDistinct("origin_airport"), countDistinct("dest_
airport"), count("*")).show
+-------------------------------+-----------------------------+---------+
| count(DISTINCT origin_airport)| count(DISTINCT dest_airport)| count(1)|
+-------------------------------+-----------------------------+---------+
| 322| 322| 4693|


========================================================================================================================================================================================================================================================================================

Getting the Minimum and Maximum Values of the count Column
flight_summary.select(min("count"), max("count")).show
+-----------+-----------+
| min(count)| max(count)|
+-----------+-----------+
| 1| 13744|
+-----------+-----------+

Using the sum Function to Sum Up the count Values
flight_summary.select(sum("count")).show

Using the sumDistinct Function to Sum Up the Distinct count Values
flight_summary.select(sumDistinct("count")).show

Computing the Average Value of the count Column Using Two
Different Ways
flight_summary.select(avg("count"), (sum("count") / count("count"))).show


============================================================================================================================================


===================================== Main imp grouping and counting

Grouping by origin_airport and Performing a count Aggregation
flight_summary.groupBy("origin_airport").count().show(5, false)


Grouping by origin_state and origin_city and Performing a Count
Aggregation
flight_summary.groupBy('origin_state, 'origin_city).count
.where('origin_state === "CA").orderBy('count.desc).show(5)
+-------------+-----------------+-------+
| origin_state| origin_city| count|
+-------------+-----------------+-------+
| CA| San Francisco| 80|
| CA| Los Angeles| 80|
============================================================================================================================================

============= Join Operations

// create two dataframes

// register them as views so we can use SQL for perform joins
employeeDF.createOrReplaceTempView("employees")
deptDF.createOrReplaceTempView("departments")

========== ineer join

// using SQL
spark.sql("select * from employees JOIN departments on dept_no == id").show

Different Ways of Expressing a Join Expression
// a shorter version of the join expression
employeeDF.join(deptDF, 'dept_no === 'id).show
// specify the join expression inside the join transformation
employeeDF.join(deptDF, employeeDF.col("dept_no") === deptDF.col("id")).show
// specify the join expression using the where transformation
employeeDF.join(deptDF).where('dept_no === 'id).show



============== Left Outer join

Performing a Left Outer Join
// the join type can be either "left_outer" or "leftouter"
employeeDF.join(deptDF, 'dept_no === 'id, "left_outer").show
// using SQL
spark.sql("select * from employees LEFT OUTER JOIN departments on dept_no
== id").show

============ Right outer join

Performing a Right Outer Join
employeeDF.join(deptDF, 'dept_no === 'id, "right_outer").show
// using SQL
spark.sql("select * from employees RIGHT OUTER JOIN departments on dept_no
== id").show


============ Outer Join or Full join

Performing an Outer Join
employeeDF.join(deptDF, 'dept_no === 'id, "outer").show
// using SQL
spark.sql("select * from employees FULL OUTER JOIN departments on dept_no
== id").show



============================================================================================================================================

===========

========================= Spark -SQL from my scripts developed for Data Engineering

// reading the data 

val df = spark.read.option("header","true").option("delimiter","|").format("csv").load("adl://eaasedlstgadls.azuredatalakestore.net/data/mo/quality_engine/om_output/")

// getting null value count in the columns
println("LOG:col_1  :"+ df.filter('col_1.isNull).count)
println("LOG:col_2 :"+  df.filter('col_2.isNull).count)

println("LOG:id  :"+spark.sql("select id  from quality_engine_rpt.uv_master where id  IS NULL").count)
println("LOG:esn  :"+spark.sql("select esn  from quality_engine_rpt.uv_master where esn  IS NULL").count)



// filltering for two conditions and getting null count of the columns
println("LOG:col_2  :"+ df.filter('col_1.isNull && 'type==="B" ).count)
println("LOG:col_2  :"+ df.filter('col_2.isNull && 'type==="B" ).count)


/// Getting the Duplicate value count 

println("LOG: Table Name: Duplication Count")
println("LOG: wb_output_build_month: "+spark.sql("select count(*) from(select concat(rel_user_appl_desc,build_month,program_group_name,rel_engine_name_desc,code,calc_date) from quality_engine_mo.wb_output_build_month group by concat(rel_user_appl_desc,build_month,program_group_name,rel_engine_name_desc,code,calc_date) having count(concat(rel_user_appl_desc,build_month,program_group_name,rel_engine_name_desc,code,calc_date))>1)temp").show(false))

println("LOG: OUTPUT TABLE ------wb_output_build_year-------")

println("LOG: wb_output_build_year : "+spark.sql("select count(*) from(select concat(rel_user_appl_desc,build_year,program_group_name,rel_engine_name_desc,code,calc_date) from quality_engine_mo.wb_output_build_year group by concat(rel_user_appl_desc,build_year,program_group_name,rel_engine_name_desc,code,calc_date) having count(concat(rel_user_appl_desc,build_year,program_group_name,rel_engine_name_desc,code,calc_date))>1)temp").show(false))

println("LOG: OUTPUT TABLE ------wb_output_inservice_date-------")

println("LOG: wb_output_inservice_date :"+spark.sql("select count(*) from(select engine_serial_num  from quality_engine_mo.wb_output_inservice_date group by engine_serial_num having count(engine_serial_num)>1)temp").show(false))

println("LOG: OUTPUT TABLE ------om_output-------")

val df1=spark.read.option("header","true").option("delimiter","|").format("csv").load("adl://eaasedlstgadls.azuredatalakestore.net/data/mo/quality_engine/om_output/")
println("LOG: om_output :"+dif1.groupBy("om_key","calc_date").count.filter($"count">1).count)



println("LOG: OUTPUT TABLE ------spi_main_table-------")
val df4=spark.read.option("header","true").option("delimiter","|").format("csv").load("adl://eaasedlstgadls.azuredatalakestore.net/data/mo/quality_engine/spi_main_table/")

println("LOG: spi_main_table :"+df4.groupBy("combinations","fail_code","engine_series").count.filter($"count">1).count)

println("LOG: OUTPUT TABLE ------spi_miles_bucket-------")

val df5=spark.read.option("header","true").option("delimiter","|").format("csv").load("adl://eaasedlstgadls.azuredatalakestore.net/data/mo/quality_engine/spi_miles_bucket/")

println("LOG: spi_miles_bucket :"+df5.groupBy("miles","eng_ty","appl_grp").count.filter($"count">1).count)


========================================================================================================================================================================================================================================================================================

============== Pyspark Notes

wb_out= spark.read.option("header","true").option("delimiter",",").option("inferSchema", "true").format("csv").load("/home/qx816/notebooks/Wb_may8/data/model_input/raw/weibull_b/build.csv")

wb_out_1= spark.read.option("header","true").option("delimiter",",").option("inferSchema", "true").format("csv").load("/home/qx816/notebooks/Wb_may8/data/model_input/raw/weibull_b/claims.csv")

failm_ou1.printSchema()
failm_ou1.count()

sqlContext.registerDataFrameAsTable(failm_ou1, "failm_ou1")
sqlContext.registerDataFrameAsTable(fault_ou2, "fault_ou2")

ck = sqlContext.sql("select max(BUILD_DATE) as Max, min(BUILD_DATE) as Min from failm_ou1")

ck.show()


============================================================================================================================================
=====================================================================================================================================


Sources ::::
https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/

https://creativedata.atlassian.net/wiki/spaces/SAP/pages/31162397/Add+a+Python+job

https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Column.html

--====================== TO OPEN THE SPARK CONSOLE AND SPARK SCRIPTING ===========================================================================================

spark-shell --conf spark.ui.port=2021<<EOF

println("LOG: OUTPUT TABLE ------om_output-------")

val df = spark.read.option("header","true").option("delimiter","|").format("csv").load("adl://eaasedlstgadls.azuredatalakestore.net/data/mo/quality_engine/om_output/")

df.groupBy("combinations","fail_code","engine_series").count.filter($"count">1).count

EOF

=========== Schema Check =============================================================================================

df.printSchema  ======= in spark here df is table or data frame

=================   logic TO eleminate header in Spark ==============================================================================================

val df = spark.read.option("header","true").option("delimiter","|").format("csv").load("adl://eaasedlstgadls.azuredatalakestore.net/data/mo/quality_engine/spi_miles_bucket/")

df.filter('ANALYTICS_RUN_DATE.isNull).count
=====================================================================================================================================
================================= Filtering in Spark and Pyspark
df.filter('ANALYTICS_RUN_DATE.isNull).count

df.filter('engine_ser,'hcasd,awddfjb).unique)

df_st.select("cost_nbr_population").distinct.show(50,false)

=========== TO CHECKS DUPLICATES =============================================================================================

println("LOG: öm_output duplication OUTPUT TABLE  ::  " +df.groupBy("calc_date","om_key").count.filter($"count">1).count)

println("LOG: öm_output duplication OUTPUT TABLE  ::  " +df.groupBy("calc_date","om_key").count.filter($"count">1).count)

df.groupBy("combinations","fail_code","engine_series").count.filter($"count">1).count

============================== Grouping by two columns ===========================================================================

df.filter('engine_ser,'hcasd,awddfjb).unique)

df.groupBy("x","y").count().filter("count >= 2").show() 

======================================================================================================================================================
----------- For min and amx
om_output_df.groupBy("ENGINE_SERIES").agg(max("CALC_DATE"), min("CALC_DATE")).show()



df.agg(min("A"), max("A")).show()

--------------------------------------- TO getting Max values by grouping by other columns

val windowSpec = df_ot.partitionBy(myDF("CALC_DATE")).orderBy(myDF("CALC_DATE").desc)

myDF.withColumn("max_CALC_DATE", first(myDF("CALC_DATE")).over(windowSpec).as("max_CALC_DATE")).filter("CALC_DATE = max_CALC_DATE")

--------------- for distinct values

df_ot.select('REL_OEM_NORMALIZED_GROUP).distinct.show()

----------- For max and Min dates 
df.select(max("CALC_DATE")).show() 

=====================================================================================================================================


Collecting the unique values into the list in spark dataframe ::


Pre_X_W_df.select('REL_CMP_ENGINE_NAME').distinct().rdd.map(lambda r: r[0]).collect()


converting the column values into different datatypes :


=====================================================================================================================================



======================================================================================================================================================
=====================================================================================================================================================
================================== Jupyter Notebook Pyspark

from pyspark.sql import SparkSession, HiveContext
spark = SparkSession.builder.appName('Ops').getOrCreate()

from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, IntegerType, StringType

import pyspark
import pandas as pd

wb_out= spark.read.option("header","true").option("delimiter",",").option("inferSchema", "true").format("csv").load("/home/qx816/notebooks/Wb_may8/data/model_input/raw/weibull_b/build.csv")

wb_out_1= spark.read.option("header","true").option("delimiter",",").option("inferSchema", "true").format("csv").load("/home/qx816/notebooks/Wb_may8/data/model_input/raw/weibull_b/claims.csv")

failm_ou1.printSchema()
failm_ou1.count()

sqlContext.registerDataFrameAsTable(failm_ou1, "failm_ou1")
sqlContext.registerDataFrameAsTable(fault_ou2, "fault_ou2")

ck = sqlContext.sql("select max(BUILD_DATE) as Max, min(BUILD_DATE) as Min from failm_ou1")

ck.show()

co=sqlContext.sql("select OM_CRRT_PRIORITY_SCORE from om_out_df where om_algo == 'MA_Ribbon' and cast(OM_CRRT_PRIORITY_SCORE as int)<0 ")

co.count()

co=sqlContext.sql("select OM_SCORE_REL_BUCKET,count(CALC_ID) from om_out_df where calc_date == '2019-03-07' GROUP BY OM_SCORE_REL_BUCKET ")

cd=sqlContext.sql("select distinct(REL_BUILD_DATE) from ff_csv_1")

====================================== To save the results into csv (to_ press tab we will get options for file saving formates)
cdf = cd.toPandas()

cdf.to_csv('Wb_L_file_May_2_Gold.csv')

=====================================================================================================================================

===================== To print date which is 7 days past present date in scala

spark-shell --conf spark.ui.port=2021<<EOF

import java.util.{Date,Calendar}

def getBeforeDate(maxDate: java.util.Date,days: Int): String = {
    var maxcal: Calendar = Calendar.getInstance()
    maxcal.setTime(maxDate)
    maxcal.add(Calendar.DATE, - days)
    val dateStringBefore48Weeks_X = maxcal.get(Calendar.YEAR)  + "-" + ( maxcal.get(Calendar.MONTH) +1) + "-" + maxcal.get(Calendar.DATE)
    dateStringBefore48Weeks_X
  }


val date= new Date

getBeforeDate(date,7)

=====================================================================================================================================


===================== Note starts ======
./bin/pyspark --packages com.databricks:spark-csv_2.10:1.3.0
=== Reading csv files

train = sqlContext.load(source="com.databricks.spark.csv", path = 'PATH/train.csv', header = True,inferSchema = True)
test = sqlContext.load(source="com.databricks.spark.csv", path = 'PATH/test-comb.csv', header = True,inferSchema = True)

-=================to see datatype of columns
train.printSchema()
-================ to Show first n observation
train.head(5)
==================To see the result in more interactive manner (rows under the columns)
train.show(2,truncate= True)
====================  to Count the number of rows in DataFrame
train.count(),test.count()
=================How many columns do we have in train and test files along with their names
len(train.columns), train.columns
len(test.columns), test.columns

-============== How to get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame
train.describe().show()

============= column wise
train.describe('Product_ID').show()

=============== How to select column(s) from the DataFrame
train.select('User_ID','Age').show(5)

-============== How to find the number of distinct product in train and test files
train.select('Product_ID').distinct().count()
test.select('Product_ID').distinct().count()
-=======================
Let us check what are the categories for Product_ID, which are in test file but not in train file by applying subtract operation.We can do the same for all categorical features.

diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))
diff_cat_in_train_test.distinct().count()

-============= What if I want to calculate pair wise frequency of categorical columns
We can use crosstab operation on DataFrame to calculate the pair wise frequency of columns. Let’s apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame
train.crosstab('Age', 'Gender').show()
------===== What If I want to get the DataFrame which won’t have duplicate rows of given DataFrame
train.select('Age','Gender').dropDuplicates().show()
--=========== What if I want to drop the all rows with null value
The dropna operation can be use here. To drop row from the DataFrame it consider three options.

how– ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.
thresh – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.
subset – optional list of column names to consider.
train.dropna().count()

----== What if I want to fill the null values in DataFrame with constant number
Use fillna operation here. The fillna will take two parameters to fill the null values.

value:
 It will take a dictionary to specify which column will replace with which value.
 A value (int , float, string) for all columns.
subset: Specify some selected columns.
Let’s fill ‘-1’ inplace of null values in train DataFrame.
train.fillna(-1).show(2)
-----=========== If I want to filter the rows in train which has Purchase more than 15000


-----==============How to Apply SQL Queries on DataFrame

train.registerAsTable('train_table')

sqlContext.sql('select Product_ID from train_table').show(5)
sqlContext.sql('select Age, max(Purchase) from train_table group by Age').show()



=====================================================================================================================================




=====================================================================================================================================




=====================================================================================================================================




=====================================================================================================================================




=====================================================================================================================================





=====================================================================================================================================





=====================================================================================================================================





============================================================================================================================================


============================================================================================================================================
