
# To use into hive environment

/usr/bin/hive

// hive query 

select * from database.table limit 10;

=====================================================================================================================================

# Creation of Shell scripts in hive 

/usr/bin/hive <<EOF

println("LOG: OUTPUT TABLE -------")

println("LOG:col  :"+ select count(*)  from database.table where col IS NULL)

EOF

# Hive shell script to check the error log in the log files created in the HDFS path 

if grep -l error *.log; then
        echo "THERE ARE ERRORS IN THE EXTRACT"
fi

=====================================================================================================================================

To Run .hql files from hive ::(.hql files you can write sql commands)

hive -f /home/<path to hive file>/filename.hql
hive -f /home/<path to hive file>/filename.hql

=====================================================================================================================================
=====================================================================================================================================

# Hadoop commands

# == To list out the directories and files present in this path
hadoop fs -ls adl://<azure data lake storge>/data/mo/qa

=====================================================================================================================================

# Moving the large datasets between two clueters in hadoop using distcp  
#move current dbs to restore folder (with renaming)
#current DB -> Restore Folder 

hadoop distcp $current_folder_path/quality_compute.db $restore_folder_path/quality_compute_old.db
hadoop distcp $current_folder_path/quality_primary.db $restore_folder_path/quality_primary_old.db

=====================================================================================================================================


# Cleaning the current folder after moving
# clean up current folder
#delete files from existing location

hadoop fs -rm -r $current_folder_path/quality_compute_old.db
hadoop fs -rm -r $current_folder_path/quality_primary_old.db


=====================================================================================================================================

# For Schema check

describe tablename; 

# For tables in Database

describe databasename

=====================================================================================================================================

# Select funtion in hive 

select num,
    collectUDAF(id, values) as new_map 
from
    ( SELECT num,id,collect_set(value) as values FROM tbl GROUP BY num,id ) as sub GROUP BY num

=====================================================================================================================================
// TO know the usage of keys in hive we can get description of keys as below

$ /usr/bin/hive -H
usage: hive
 -d,--define <key=value>          Variable subsitution to apply to hive
                                  commands. e.g. -d A=B or --define A=B
    --database <databasename>     Specify the database to use
 -e <quoted-query-string>         SQL from command line
 -f <filename>                    SQL from files
 -H,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
    --hivevar <key=value>         Variable subsitution to apply to hive
                                  commands. e.g. --hivevar A=B
 -i <filename>                    Initialization SQL file
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the
                                  console)
                                  
=====================================================================================================================================
                                
# TO DELETE ALL THE LINES IN SCRIPT :::	

In vi I use

:%d
where

: tells vi to go in command mode
% means all the line
d : delete
On the command line,


