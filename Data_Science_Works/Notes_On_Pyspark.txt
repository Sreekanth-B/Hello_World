

The key data type used in PySpark is the Spark dataframe. This object can be thought of as a table distributed across a cluster and has functionality 
that is similar to dataframes in R and Pandas. If you want to do distributed computation using PySpark, then you’ll need to perform operations on Spark 
dataframes, and not other python data types.

It is also possible to use Pandas dataframes when using Spark, by calling toPandas() on a Spark dataframe, which returns a pandas object. 
However, this function should generally be avoided except when working with small dataframes, because it pulls the entire object into memory on a single node.

One of the key differences between Pandas and Spark dataframes is eager versus lazy execution. 
In PySpark, operations are delayed until a result is actually needed in the pipeline. 
For example, you can specify operations for loading a data set from S3 and applying a number of transformations to the dataframe, 
but these operations won’t immediately be applied. Instead, a graph of transformations is recorded, and once the data is actually needed, 
for example when writing the results back to S3, then the transformations are applied as a single pipeline operation. 
This approach is used to avoid pulling the full data frame into memory and enables more effective processing across a cluster of machines. 
With Pandas dataframes,everything is pulled into memory, and every Pandas operation is immediately applied.

Reading data ::

One of the first steps to learn when working with Spark is loading a data set into a dataframe. Once data has been loaded into a dataframe, 
you can apply transformations, perform analysis and modeling, create visualizations, and persist the results. 
In Python, you can load files directly from the local file system using Pandas:

import pandas as pd
pd.read_csv("dataset.csv")

Databricks file system (DBFS), which provides paths in the form of /FileStore. The first step is to upload the CSV file you’d like to process.

The next step is to read the CSV file into a Spark dataframe as shown below. This code snippet specifies the path of the CSV file, 
and passes a number of arguments to the read function to process the file.The last step displays a subset of the loaded dataframe, similar to df.head() in Pandas.

file_location = "/FileStore/tables/game_skater_stats.csv"
df = spark.read.format("csv").option("inferSchema", 
           True).option("header", True).load(file_location)
display(df)

I prefer using the parquet format when working with Spark, because it is a file format that includes metadata about the column data types, 
offers file compression, and is a file format that is designed to work well with Spark. AVRO is another format that works well with Spark. 
The snippet below shows how to take the dataframe from the past snippet and save it as a parquet file on DBFS, 
and then reload the dataframe from the saved parquet file.

# DBFS

df.write.save('/FileStore/parquet/game_skater_stats',  
               format='parquet')
df = spark.read.load("/FileStore/parquet/game_skater_stats")
display(df)

The result of this step is the same, but the execution flow is significantly different. When reading CSV files into dataframes, 
Spark performs the operation in an eager mode, meaning that all of the data is loaded into memory before the next step begins execution, 
while a lazy approach is used when reading files in the parquet format. Generally, you want to avoid eager operations when working with Spark, 
and if I need to process large CSV files I’ll first transform the data set to parquet format before executing the rest of the pipeline.

Often you’ll need to process a large number of files, such as hundreds of parquet files located at a certain path or directory in DBFS. 
With Spark, you can include a wildcard in a path to process a collection of files. For example, you can load a batch of parquet files from S3 as follows:

# S3 
df = spark.read .load("s3a://my_bucket/game_skater_stats/*.parquet")

This approach is useful if you have a seperate parquet file per day, or if there is a prior step in your pipeline that outputs hundreds of parquet files.

Writing Data ::

Similar to reading data with Spark, it’s not recommended to write data to local storage when using PySpark. Instead, you should used a distributed file
 system such as S3 or HDFS. If you going to be processing the results with Spark, then parquet is a good format to use for saving data frames. 
 The snippet below shows how to save a dataframe to DBFS and S3 as parquet.

# DBFS (Parquet)
df.write.save('/FileStore/parquet/game_stats',format='parquet')

# S3 (Parquet)
df.write.parquet("s3a://my_bucket/game_stats", mode="overwrite")

 When building predictive models with PySpark and massive data sets, MLlib is the preferred library because it natively operates on Spark dataframes. 
In order to use one of the supervised algorithms in MLib, you need to set up your dataframe with a vector of features and a label as a scalar. 
Once prepared, you can use the fit function to train the model. The snippet below shows how to combine several of the columns in the dataframe 
into a single features vector using a VectorAssembler. We use the resulting dataframe to call the fit function and then generate summary statistics for the model.

# Source :: Source :: https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873

